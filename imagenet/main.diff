diff --git a/imagenet/main.py b/imagenet/main.py
index e828ea0..22bd88b 100644
--- a/imagenet/main.py
+++ b/imagenet/main.py
@@ -20,6 +20,7 @@ import torchvision.models as models
 import torchvision.transforms as transforms
 from torch.optim.lr_scheduler import StepLR
 from torch.utils.data import Subset
+from torch.utils.tensorboard import SummaryWriter
 
 model_names = sorted(name for name in models.__dict__
     if name.islower() and not name.startswith("__")
@@ -142,7 +143,7 @@ def main_worker(gpu, ngpus_per_node, args):
         model = models.__dict__[args.arch](pretrained=True)
     else:
         print("=> creating model '{}'".format(args.arch))
-        model = models.__dict__[args.arch]()
+        model = models.__dict__[args.arch](num_classes=200)
 
     if not torch.cuda.is_available() and not torch.backends.mps.is_available():
         print('using CPU, this will be slow')
@@ -196,7 +197,7 @@ def main_worker(gpu, ngpus_per_node, args):
                                 weight_decay=args.weight_decay)
     
     """Sets the learning rate to the initial LR decayed by 10 every 30 epochs"""
-    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)
+    scheduler = StepLR(optimizer, step_size=5, gamma=0.1)
     
     # optionally resume from a checkpoint
     if args.resume:
@@ -221,22 +222,27 @@ def main_worker(gpu, ngpus_per_node, args):
         else:
             print("=> no checkpoint found at '{}'".format(args.resume))
 
+    writer_train = SummaryWriter('runs/exp1/train')
+    writer_train_acc1=SummaryWriter('runs/exp1/trainacc1')
+    writer_train_acc5=SummaryWriter('runs/exp1/trainacc5')
+    writer_val= SummaryWriter('runs/exp1/val')
+    writer_val_acc1= SummaryWriter('runs/exp1/valacc1')
+    writer_val_acc5= SummaryWriter('runs/exp1/valacc5')
 
     # Data loading code
     if args.dummy:
         print("=> Dummy data is used!")
-        train_dataset = datasets.FakeData(1281167, (3, 224, 224), 1000, transforms.ToTensor())
-        val_dataset = datasets.FakeData(50000, (3, 224, 224), 1000, transforms.ToTensor())
+        train_dataset = datasets.FakeData(1281167, (3, 64, 64), 200, transforms.ToTensor())
+        val_dataset = datasets.FakeData(50000, (3, 64, 64), 200, transforms.ToTensor())
     else:
-        traindir = os.path.join(args.data, 'train')
-        valdir = os.path.join(args.data, 'val')
+        traindir = './imagenet/train'
+        valdir = './imagenet/val_reorg'
         normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                      std=[0.229, 0.224, 0.225])
 
         train_dataset = datasets.ImageFolder(
             traindir,
             transforms.Compose([
-                transforms.RandomResizedCrop(224),
                 transforms.RandomHorizontalFlip(),
                 transforms.ToTensor(),
                 normalize,
@@ -245,8 +251,6 @@ def main_worker(gpu, ngpus_per_node, args):
         val_dataset = datasets.ImageFolder(
             valdir,
             transforms.Compose([
-                transforms.Resize(256),
-                transforms.CenterCrop(224),
                 transforms.ToTensor(),
                 normalize,
             ]))
@@ -267,18 +271,19 @@ def main_worker(gpu, ngpus_per_node, args):
         num_workers=args.workers, pin_memory=True, sampler=val_sampler)
 
     if args.evaluate:
-        validate(val_loader, model, criterion, args)
+        validate(391.0/40.0, val_loader, model, criterion, 0, args, writer_val, writer_val_acc1, writer_val_acc5)
         return
 
     for epoch in range(args.start_epoch, args.epochs):
         if args.distributed:
             train_sampler.set_epoch(epoch)
 
+        r=len(train_loader)/len(val_loader)
         # train for one epoch
-        train(train_loader, model, criterion, optimizer, epoch, device, args)
+        train(train_loader, model, criterion, optimizer, epoch, device, args, writer_train, writer_train_acc1, writer_train_acc5)
 
         # evaluate on validation set
-        acc1 = validate(val_loader, model, criterion, args)
+        acc1 = validate(r, val_loader, model, criterion, epoch, args, writer_val, writer_val_acc1, writer_val_acc5)
         
         scheduler.step()
         
@@ -296,9 +301,18 @@ def main_worker(gpu, ngpus_per_node, args):
                 'optimizer' : optimizer.state_dict(),
                 'scheduler' : scheduler.state_dict()
             }, is_best)
+        if epoch % 10 ==5:
+            torch.save({
+                'epoch': epoch + 1,
+                'arch': args.arch,
+                'state_dict': model.state_dict(),
+                'best_acc1': best_acc1,
+                'optimizer' : optimizer.state_dict(),
+                'scheduler' : scheduler.state_dict()
+                }, './output/checkpoint%d.pth.tar'%(epoch))
 
 
-def train(train_loader, model, criterion, optimizer, epoch, device, args):
+def train(train_loader, model, criterion, optimizer, epoch, device, args, writer, writeracc1, writeracc5):
     batch_time = AverageMeter('Time', ':6.3f')
     data_time = AverageMeter('Data', ':6.3f')
     losses = AverageMeter('Loss', ':.4e')
@@ -308,6 +322,7 @@ def train(train_loader, model, criterion, optimizer, epoch, device, args):
         len(train_loader),
         [batch_time, data_time, losses, top1, top5],
         prefix="Epoch: [{}]".format(epoch))
+    running_loss=0.0
 
     # switch to train mode
     model.train()
@@ -340,15 +355,27 @@ def train(train_loader, model, criterion, optimizer, epoch, device, args):
         batch_time.update(time.time() - end)
         end = time.time()
 
+        running_loss=running_loss+loss.item()
         if i % args.print_freq == 0:
             progress.display(i + 1)
+            writer.add_scalar('loss',
+                            running_loss / args.print_freq,
+                            epoch * len(train_loader) + i)
+            writeracc1.add_scalar('training acc',
+                            acc1[0],
+                            epoch * len(train_loader) + i)
+            writeracc5.add_scalar('training acc',
+                            acc5[0],
+                            epoch * len(train_loader) + i)
+        running_loss=0.0
 
 
-def validate(val_loader, model, criterion, args):
+def validate(rate, val_loader, model, criterion, epoch, args, writer, writeracc1, writeracc5):
 
     def run_validate(loader, base_progress=0):
         with torch.no_grad():
             end = time.time()
+            running_loss=0.0
             for i, (images, target) in enumerate(loader):
                 i = base_progress + i
                 if args.gpu is not None and torch.cuda.is_available():
@@ -373,8 +400,19 @@ def validate(val_loader, model, criterion, args):
                 batch_time.update(time.time() - end)
                 end = time.time()
 
+                running_loss=running_loss+loss.item()
                 if i % args.print_freq == 0:
                     progress.display(i + 1)
+                    writer.add_scalar('loss',
+                                      running_loss / args.print_freq,
+                                      (epoch * len(val_loader) + i+10)*rate)
+                    writeracc1.add_scalar('val acc',
+                                          acc1[0],
+                                          epoch * len(val_loader) + i)
+                    writeracc5.add_scalar('val acc',
+                                          acc5[0],
+                                          epoch * len(val_loader) + i)
+                    running_loss=0.0
 
     batch_time = AverageMeter('Time', ':6.3f', Summary.NONE)
     losses = AverageMeter('Loss', ':.4e', Summary.NONE)
@@ -509,3 +547,40 @@ def accuracy(output, target, topk=(1,)):
 
 if __name__ == '__main__':
     main()
+
+#The code below can reorganize the validation set.
+
+#import os
+#import shutil
+#path=r"C:\Users\Dido\Documents\GitHub\PythonHW2\examples\imagenet\imagenet\val"
+#path_re=r"C:\Users\Dido\Documents\GitHub\PythonHW2\examples\imagenet\imagenet\val_reorg"
+#if not os.path.exists(path_re):
+#    os.makedirs(path_re)
+#namelist=[]
+#val1=[]
+#val2=[]
+#val3=[]
+#val4=[]
+#with open(path+'\\val_annotations.txt',"r",encoding="UTF-8") as f:
+#    data=f.readlines()
+#    for d in data:
+#        save=d.split('\t')
+#        namelist.append(save[1])
+#        val1.append(save[2])
+#        val2.append(save[3])
+#        val3.append(save[4])
+#        val4.append(save[5].replace("\n",""))
+#for i in range(0,10000):
+#    if not os.path.exists(path_re+'\\%s'%(namelist[i])):
+#        os.makedirs(path_re+'\\%s'%(namelist[i]))
+#        os.makedirs(path_re+'\\%s\\images'%(namelist[i]))
+#        with open(path_re+'\\%s./%s_boxes.txt'%(namelist[i],namelist[i]),'w',encoding="UTF-8") as f:
+#            f.write('%s_0.JPEG\t%s\t%s\t%s\t%s'%(namelist[i],val1[i],val2[i],val3[i],val4[i]))
+#        shutil.copy(path+'\\images./val_%d.JPEG'%(i),path_re+'\\%s\\images'%(namelist[i]))
+#        os.rename(path_re+'\\%s\\images./val_%d.JPEG'%(namelist[i],i),path_re+'\\%s\\images./%s_0.JPEG'%(namelist[i],namelist[i]))
+#    else:
+#        amount=len(os.listdir(path_re+'\\%s\\images'%(namelist[i])))
+#        with open(path_re+'\\%s./%s_boxes.txt'%(namelist[i],namelist[i]),'a',encoding="UTF-8") as f:
+#            f.write('\n%s_%d.JPEG\t%s\t%s\t%s\t%s'%(namelist[i],amount,val1[i],val2[i],val3[i],val4[i]))
+#        shutil.copy(path+'\\images./val_%d.JPEG'%(i),path_re+'\\%s\\images'%(namelist[i]))
+#        os.rename(path_re+'\\%s\\images./val_%d.JPEG'%(namelist[i],i),path_re+'\\%s\\images./%s_%d.JPEG'%(namelist[i],namelist[i],amount))
\ No newline at end of file
